# ğŸ§  50 Days of Deep Learning

**Finish Your 2025 Resolution: Master Deep Learning by December 31st, 2025**

A comprehensive, day-by-day deep learning course covering everything from neural network basics to transformers and modern LLMs.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.5+](https://img.shields.io/badge/PyTorch-2.5+-orange.svg)](https://pytorch.org/)

## ğŸ¯ The Commitment

**The course starts on November 11th and will finish on December 31st, 2025. Let's finish this year's resolutionâ€”together.**

This year, let's actually complete what we started. No more abandoned courses. Join us starting November 11th, 2025. Just 1-2 hours daily, and you'll finish by December 31st, 2025.

## ğŸ“š Course Overview

This 50-day course is designed to take you from deep learning fundamentals to advanced transformer architectures. Each day builds upon previous concepts, with hands-on exercises and practical projects.

**Start Date:** November 11th, 2025  
**Time Commitment:** 1-2 hours per day  
**Deadline:** December 31st, 2025  
**Goal:** Complete your 2025 resolution

### Course Structure

The course is organized into **7 weeks** covering different themes:

- **Week 1**: The Absolute Basics
- **Week 2**: Building Your First Practical Model
- **Week 3**: Deep Learning for Images (CNNs)
- **Week 4**: Deep Learning for Text (RNNs)
- **Week 5**: The Bridge to Transformers (Seq2Seq & Attention)
- **Week 6**: The Transformer Architecture
- **Week 7**: The Models That Changed the World (BERT & GPT)

## ğŸ¯ Learning Objectives

By the end of this course, you will:

- Understand the fundamentals of neural networks and deep learning
- Build and train your first deep learning models
- Implement CNNs for image classification
- Work with RNNs, LSTMs, and GRUs for sequence data
- Understand and implement attention mechanisms
- Build transformer models from scratch
- Fine-tune pre-trained models like BERT and GPT
- Deploy deep learning models in production

## ğŸ“‹ Prerequisites

- Basic Python programming skills
- Understanding of linear algebra and calculus (helpful but not required)
- Familiarity with NumPy and basic data manipulation
- Enthusiasm to learn! ğŸš€

## ğŸš€ Getting Started: Join Us Starting November 11th, 2025

> **Course starts:** November 11th, 2025  
> **Course finishes:** December 31st, 2025  
> **Join us on launch day to complete your resolution this year!**

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/50-days-deep-learning.git
   cd 50-days-deep-learning
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up Jupyter Notebook** (if not already installed)
   ```bash
   pip install jupyter
   jupyter notebook
   ```

5. **Start Day 1 today!** Open `daily/day 1/README.md` and begin your journey.

For detailed setup instructions, see [docs/installation.md](docs/installation.md).

**Commitment:** Spend 1-2 hours daily, and you'll finish by December 31st, 2025. This year, let's actually complete it.

## ğŸ“– Course Schedule

### Week 1: The Absolute Basics
- [Day 1: What is Deep Learning?](daily/day%201/) - Introduction to Deep Learning
- [Day 2: The Simplest "Brain"](daily/day%202/) - The Perceptron Explained
- [Day 3: From One to Many](daily/day%203/) - Introduction to Neural Networks
- [Day 4: How Do Neural Networks Learn?](daily/day%204/) - A Simple Guide to Backpropagation
- [Day 5: The "On/Off Switch"](daily/day%205/) - Understanding Activation Functions
- [Day 6: Measuring Mistakes](daily/day%206/) - A Beginner's Guide to Loss Functions
- [Day 7: The Learning Engine](daily/day%207/) - Introduction to Optimizers

### Week 2: Building Your First Practical Model
- [Day 8: Your First "Hello World"](daily/day%208/) - MNIST with Keras/PyTorch
- [Day 9: The Big Problem](daily/day%209/) - What is Overfitting?
- [Day 10: Fighting Overfitting (Part 1)](daily/day%2010/) - Introduction to Regularization
- [Day 11: Fighting Overfitting (Part 2)](daily/day%2011/) - Understanding Dropout
- [Day 12: The Art of Tuning](daily/day%2012/) - What Are Hyperparameters?
- [Day 13: Better, Faster Training](daily/day%2013/) - The Power of Batch Normalization
- [Day 14: A Better Optimizer](daily/day%2014/) - Why Everyone Uses "Adam"

### Week 3: Deep Learning for Images (CNNs)
- [Day 15: Why Can't a Basic Neural Network "See"?](daily/day%2015/) - The Limitations of MLPs
- [Day 16: The Building Block of Vision](daily/day%2016/) - The Convolution Layer Explained
- [Day 17: Shrinking the Image](daily/day%2017/) - Understanding Pooling Layers
- [Day 18: Putting It All Together](daily/day%2018/) - Building Your First CNN
- [Day 19: A Look at History](daily/day%2019/) - The Architecture of LeNet-5
- [Day 20: The "Shortcut" to Success](daily/day%2020/) - What is Transfer Learning?
- [Day 21: Practical Project](daily/day%2021/) - Using Pre-trained VGG or ResNet

### Week 4: Deep Learning for Text (RNNs)
- [Day 22: The Challenge of "Sequence"](daily/day%2022/) - Introduction to Time Series and Text Data
- [Day 23: Networks with "Memory"](daily/day%2023/) - The Recurrent Neural Network (RNN)
- [Day 24: The Problem with RNNs](daily/day%2024/) - Vanishing and Exploding Gradients
- [Day 25: The Solution](daily/day%2025/) - Long Short-Term Memory (LSTM) Networks
- [Day 26: A Simpler Alternative](daily/day%2026/) - The Gated Recurrent Unit (GRU)
- [Day 27: Reading Both Ways](daily/day%2027/) - The Power of Bidirectional LSTMs
- [Day 28: Practical Project](daily/day%2028/) - Sentiment Analysis with an LSTM

### Week 5: The Bridge to Transformers (Seq2Seq & Attention)
- [Day 29: How to Make Words into Numbers](daily/day%2029/) - Introduction to Word Embeddings
- [Day 30: Beyond Word2Vec](daily/day%2030/) - Understanding GloVe and fastText
- [Day 31: The Encoder-Decoder Architecture](daily/day%2031/) - Building a "Seq2Seq" Model
- [Day 32: The "Bottleneck" Problem](daily/day%2032/) - Why Seq2Seq with RNNs Is Limited
- [Day 33: The Big Idea](daily/day%2033/) - An Intuitive Guide to the "Attention" Mechanism
- [Day 34: Visualizing Attention](daily/day%2034/) - Seeing How a Model Translates a Sentence
- [Day 35: Project](daily/day%2035/) - Build a Simple Seq2Seq Model

### Week 6: The Transformer Architecture
- [Day 36: Goodbye RNNs](daily/day%2036/) - Introducing the Transformer Architecture
- [Day 37: The "Self-Attention" Mechanism](daily/day%2037/) - The Heart of the Transformer
- [Day 38: Why "Multi-Head" Attention?](daily/day%2038/) - Looking at the Same Thing in Different Ways
- [Day 39: The Transformer Encoder](daily/day%2039/) - What's Inside?
- [Day 40: The Transformer Decoder](daily/day%2040/) - How It's Different from the Encoder
- [Day 41: The "Time-Travel" Problem](daily/day%2041/) - How Positional Encodings Work
- [Day 42: Review](daily/day%2042/) - The Full Transformer, Step-by-Step

### Week 7: The Models That Changed the World (BERT & GPT)
- [Day 43: Meet BERT](daily/day%2043/) - The "Encoder-Only" Transformer
- [Day 44: How BERT is Trained](daily/day%2044/) - Understanding Masked Language Models
- [Day 45: What is "Fine-Tuning"?](daily/day%2045/) - Using BERT for Text Classification
- [Day 46: Meet GPT](daily/day%2046/) - The "Decoder-Only" Transformer
- [Day 47: How GPT is Trained](daily/day%2047/) - Causal Language Modeling
- [Day 48: The Rise of Large Language Models](daily/day%2048/) - What GPT-3 Taught Us
- [Day 49: BERT vs. GPT vs. T5](daily/day%2049/) - A Simple Comparison of Modern Architectures
- [Day 50: Final Project & Course Wrap-up](daily/day%2050/) - Building Your Own Model

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ LICENSE                   # License file
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ plan.csv                 # Course plan with all topics
â”‚
â”œâ”€â”€ daily/                   # Daily lesson folders
â”‚   â”œâ”€â”€ day 1/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ outputs/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                    # Shared datasets
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”‚
â”œâ”€â”€ models/                  # Saved models
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ trained/
â”‚   â””â”€â”€ pretrained/
â”‚
â”œâ”€â”€ utils/                   # Utility functions
â”œâ”€â”€ scripts/                 # Helper scripts
â”œâ”€â”€ projects/                # Larger projects
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ syllabus.md
â”‚   â”œâ”€â”€ installation.md
â”‚   â””â”€â”€ references.md
â”‚
â””â”€â”€ assets/                  # Images, diagrams, etc.
```

## ğŸ› ï¸ Technologies Used

- **PyTorch 2.5+** - Primary deep learning framework (latest stable version)
- **Python 3.12+** - Latest Python version
- **NumPy 2.0+** - Numerical computing
- **Pandas 2.2+** - Data manipulation
- **Matplotlib/Seaborn** - Visualization
- **Jupyter Notebooks** - Interactive learning
- **Transformers** - Hugging Face transformers library (for BERT, GPT, etc.)
- **Scikit-learn** - Machine learning utilities

## ğŸ“š Resources

- [Course Syllabus](docs/syllabus.md)
- [Installation Guide](docs/installation.md)
- [References & Further Reading](docs/references.md)
- [FAQ](docs/faq.md)

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by the structure of top deep learning repositories
- Built for learners who want to master deep learning step by step

## ğŸ“Š Progress Tracking: Finish by December 31st, 2025

Keep track of your progress by checking off completed days. Remember: **The course finishes on December 31st, 2025. Let's finish this year's resolution!**

- [ ] Week 1 (Days 1-7)
- [ ] Week 2 (Days 8-14)
- [ ] Week 3 (Days 15-21)
- [ ] Week 4 (Days 22-28)
- [ ] Week 5 (Days 29-35)
- [ ] Week 6 (Days 36-42)
- [ ] Week 7 (Days 43-50)

**Tip:** Share your progress! Use #50DaysDeepLearning when you complete milestones. Accountability increases completion rates.

---

## ğŸ¯ The Finish Line: December 31st, 2025

Imagine waking up on January 1st, 2026, knowing you completed your 2025 resolution. You didn't just learn deep learningâ€”you mastered it. From perceptrons to transformers. From neural networks to GPT.

That feeling? That's worth more than any subscription course or certificate. That's the feeling of keeping a promise to yourself.

**Ready to finish your 2025 resolution?** Join us starting November 11th, 2025. Start with [Day 1](daily/day%201/) on launch day.

**The deadline is December 31st, 2025. Let's finish this year's resolutionâ€”together. ğŸš€**

*This year, let's actually do it.*

