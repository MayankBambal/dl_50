## **To Do**

**Level 1 (Product Manager Study):**
- **Video:**
- **Blog:**

**Level 2 (Junior Data Scientist):**
- **Code:**
  - [day36.ipynb](notebooks/day36.ipynb)
- **Books:**
- **Interview questions:**
  - [Easy Interview Questions](interview_questions/easy_questions.md)

**Level 3 (Data Scientist):**
- **Interview questions:**
  - [Medium Interview Questions](interview_questions/medium_questions.md)
- **Books:**

## **Plan**

**Objectives:**
- Understand why Transformers replaced RNNs
- Learn the high-level Transformer architecture
- Understand self-attention concept
- Learn about the "Attention is All You Need" paper
- Compare Transformers vs. RNNs

**Deep Learning Concept(s):**
- Transformer architecture overview
- Why Transformers: parallelization, long-range dependencies
- Self-attention vs. encoder-decoder attention
- Encoder-decoder structure
- Positional encodings (introduction)
- Multi-head attention (introduction)

**Tools Used:**
- Understanding architecture diagrams
- Preparing for implementation

**Key Learnings:**
- Understanding that Transformers use attention only (no RNNs)
- Parallel processing: all positions processed simultaneously
- Better long-range dependencies than RNNs
- Self-attention: attending to all positions in sequence
- Encoder stack and decoder stack
- Understanding that this architecture enables modern LLMs
- Foundation for BERT, GPT, T5

**References:**
- **Paper**: Vaswani, A., et al. (2017). "Attention is All You Need"
- **Book**: "Deep Learning" by Ian Goodfellow - Chapter 12 (Applications)
- **Visual Guide**: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- **Tutorial**: [Transformer Architecture Explained](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

---