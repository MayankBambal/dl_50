## **To Do**

**Level 1 (Product Manager Study):**
- **Video:**
- **Blog:**

**Level 2 (Junior Data Scientist):**
- **Code:**
  - [day48.ipynb](notebooks/day48.ipynb)
- **Books:**
- **Interview questions:**
  - [Easy Interview Questions](interview_questions/easy_questions.md)

**Level 3 (Data Scientist):**
- **Interview questions:**
  - [Medium Interview Questions](interview_questions/medium_questions.md)
- **Books:**

## **Plan**

**Objectives:**
- Understand the scale of large language models
- Learn about GPT-3 capabilities
- Understand few-shot learning
- Learn about in-context learning
- Understand scaling laws

**Deep Learning Concept(s):**
- Large language models: billions/trillions of parameters
- GPT-3: 175B parameters
- Few-shot learning: providing examples in prompt
- In-context learning: learning from examples without gradient updates
- Scaling: more parameters â†’ better performance
- Emergent abilities: capabilities that appear at scale

**Tools Used:**
- Understanding LLM capabilities
- API usage: OpenAI API (conceptual)
- Functions: understanding prompt engineering

**Key Learnings:**
- Understanding that scale matters: GPT-3 (175B) vs. GPT-2 (1.5B)
- Few-shot learning: providing 1-5 examples in prompt, model generalizes
- In-context learning: model learns task from examples without fine-tuning
- Prompting strategies: zero-shot, one-shot, few-shot
- Understanding that LLMs show emergent abilities (reasoning, code generation)
- API access: using LLMs via APIs (OpenAI, Anthropic, etc.)
- Understanding limitations: hallucinations, bias, computation cost
- Preparing for modern LLMs (GPT-4, Claude, etc.)

**References:**
- **Paper**: Brown, T., et al. (2020). "Language Models are Few-Shot Learners" (GPT-3)
- **Paper**: Kaplan, J., et al. (2020). "Scaling Laws for Neural Language Models"
- **Online**: [GPT-3 Explained](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
- **Tutorial**: [OpenAI API Documentation](https://platform.openai.com/docs)

---